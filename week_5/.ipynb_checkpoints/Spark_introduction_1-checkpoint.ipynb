{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac85279",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Apache Spark is an open-source, distributed processing system used for big data workloads.\n",
    "\n",
    "\n",
    "* It's an enhancement to Hadoop's MapReduce.\n",
    "  * Processes and retains data in memory for subsequent steps,\n",
    "  * For smaller workloads, Spark’s data processing speeds are up to 100x faster than Hadoop's MapReduce.\n",
    "\n",
    "* Spark was written in Scala\n",
    "  * Runs on the JVM\n",
    " \n",
    "* Includes libraries to support SQL queries, machine learning (MLlib), graph data analysis (GraphX) and streaming data analysis.\n",
    "  * Tools for pipeline construction and evaluation\n",
    "\n",
    "* Enahanced security.\n",
    "\n",
    "* Ideal for real-time processing as it utilizes in-memory caching and optimized query execution for fast queries against data of any size.  \n",
    "\n",
    "* Provides more operators other than map and reduce\n",
    "  * Plethora of functions for SQL-like operation, ML and working with graph data\n",
    "  * Over 80 high level operators beyond Map and Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bae26",
   "metadata": {},
   "source": [
    "### Spark and Function Programming\n",
    "\n",
    "* Spark uses functional programming to manipulate data\n",
    "  * Pradigm used in some popular languages such as Common Lisp, Scheme and Clojure, OCaml and Haskell\n",
    "  \n",
    "* Functional programming is a data oriented paradigm.\n",
    "  * Functional programming decomposes a problem into a set of functions.\n",
    "  * Logic is implemented by applying and composing functions.\n",
    "\n",
    "* It's centered around the fact that data should be manipulated by functions without maintaining any external state.\n",
    "  * No global variables\n",
    "\n",
    "  * The above defines exactly what lambda functions do.\n",
    " \n",
    "* In functional programming, we need to always return new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32a78f",
   "metadata": {},
   "source": [
    "### Spark Operations and RDDs\n",
    "\n",
    "* Provides more operators other than map and reduce\n",
    "  * Plethora of functions for SQL-like operation, ML and working with graph data\n",
    "  * Over 80 high level operators beyond Map and Reduce.\n",
    "* Provides its own distributed data framework called resilient distributed datasets or RDDs.\n",
    "   * RDD is an abstraction that represents a read-only collection of objects that are partitioned across machines.    * RDDs are fault tolerant and are accessed via parallel operations.\n",
    "\n",
    "* RDDs are cached in memory and is effective when iterating ont he same data\n",
    "  * Ex. optimization or ML algorithms\n",
    "* Fast operation speed makes it ideal for command-line-based queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31893de1",
   "metadata": {},
   "source": [
    "### Core Components of Spark\n",
    "\n",
    "![](https://www.dropbox.com/s/azebxe8nv5nsqne/spark_architecture.png?dl=1)\n",
    "* Spark Core\n",
    "  * Basic functionality: \n",
    "    * APIs that define RDDs\n",
    "    * operations and actions to process RDDs\n",
    "    \n",
    "* Spark SQL\n",
    "    APIs to interact with Apache Hive variant of SQL called Hive Query Language (HiveQL). \n",
    "    * db tables are RDD and Spark SQL queries are transformed into Spark operations\n",
    "\n",
    "* Spark Streaming\n",
    "    Enables the processing and manipulation of live streams of data in real time.\n",
    "    \n",
    "* MLlib\n",
    "  * Implementation of of machine learning algorithms using Spark on RDDs\n",
    "  * Somwhat basic algorithms for classifications, regressions,\n",
    "* GraphX\n",
    "  * Functionality for manipulating graphs and performing parallel graph operations and computations\n",
    "  * A sort of large-scale Neo4J\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bd2bd",
   "metadata": {},
   "source": [
    "### Spark Paradigm\n",
    "\n",
    "A Spark program typically follows a simple paradigm:\n",
    "\n",
    "1. The main program is the `driver` \n",
    "2. The program has one or more workers, called executors,\n",
    "  * those run code sent to them by the driver on their partitions of the RDD \n",
    "  * in local mode (or Spark in-process), the executor is the same at the driver (my laptop here)\n",
    "  \n",
    "3. Results are then sent back to the driver for aggregation or compilation.\n",
    "\n",
    "\n",
    "\n",
    "<!-- These steps are outlined as follows:\n",
    "\n",
    "    Invoke operations on the RDD by passing closures (functions) to each element of the RDD. Spark offers over \n",
    "    Use the resulting RDDs with actions (e.g. count, collect, save, etc.). Actions kick off the computing on the cluster.\n",
    "\n",
    "When Spark runs a closure on a worker, any variables used in the closure are copied to that node, but are maintained within the local scope of that closure.\n",
    "\n",
    "Spark provides two types of shared variables that can be interacted with by all workers in a restricted fashion:\n",
    "\n",
    "    Broadcast variables are distributed to all workers, but are read-only. These variables can be used as lookup tables or stopword lists.\n",
    "    Accumulators are variables that workers can “add” to using associative operations and are typically used as counters.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659f28e",
   "metadata": {},
   "source": [
    "### Spark Execution\n",
    "\n",
    "####  MISSING ???\n",
    "\n",
    "<!-- ![](https://www.dropbox.com/s/n07i9wlbqti5ptx/spark_execution.png?dl=1) -->\n",
    "\n",
    "* Interact with the Scala Interface using the PySpark Python library\n",
    "  * Rapper that uses almost exactly the same function names and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f447683",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cluster Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090af93",
   "metadata": {},
   "source": [
    "### Setting a Docker Cluster\n",
    "\n",
    "* Manually installing Spark and all its component can be a daunting task. \n",
    " * Manually deploying, configure and optimizing a Spark is complex and time consuming.\n",
    "* Easy to use Docker to install locally\n",
    "jupyter/all-spark-notebook\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "* This configuration created a master and compute nodes locally in a docker instance\n",
    "\n",
    "* There are other docker images, including (jupyter/pyspark-notebook)\n",
    "\n",
    "  * does not includ the jobs dashboard `http://localhost:4040`\n",
    "  \n",
    "*You can log into the running container using: `docker exec -it <CONTAINER_ID> bash`\n",
    "\n",
    "where <CONTAINER_ID> of the container currently running the `jupyter/all-spark-notebook` image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8309122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/20 09:38:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "# sc.stop()\n",
    "sc = SparkContext()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b7cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f872e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version is 3.1.2\n",
      "Phthon version is 3.9\n",
      "The name of the master is local[*]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version is {sc.version}\")\n",
    "\n",
    "print(f\"Phthon version is {sc.pythonVer}\")\n",
    "\n",
    "print(f\"The name of the master is {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f0394f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1632130707598'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1632130708911'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.driver.port', '36521'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '4eedec98e4d1'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab254578",
   "metadata": {},
   "source": [
    "### Creating a Text RDD\n",
    "\n",
    "* You can create RDDs in a number of ways: \n",
    "    * parallelize() function cna transform Python data structures like lists and tuples into RDDs\n",
    "    * makes the passed list fault-tolerant and distributed.\n",
    "\n",
    "\n",
    "* Another easy way to create RDDs is to read in a file with `textFile()`\n",
    "\n",
    "* Creates an RDD where every object is a line of the input text file\n",
    "\n",
    "* One an RDD is created, we can access it's `map`, `reduce` and `filter` methods\n",
    " * Those operation and other we will cover are called `transformations`\n",
    " * A transormtion on a RDD yields a new RDD \n",
    " * `flatMap` is also commonly used and is equivalent to `itertools.chain()`\n",
    " \n",
    "* To get the results of we need to perform an `action`\n",
    "  * Some operation like, `take()`, `takeSample` and `count` are needed to return data.\n",
    "  \n",
    "* More on transformations later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78163e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '485-72-3761',\n",
       " 'first_name': 'Jackson',\n",
       " 'last_name': 'Vasquez',\n",
       " 'state': 'Louisiana',\n",
       " 'zip': 91496}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insstall using the following if not already installed \n",
    "# !pip install randomuser\n",
    "from randomuser import RandomUser\n",
    "\n",
    "# Generate a single user\n",
    "user = RandomUser({\"nat\": \"us\"})\n",
    "\n",
    "def get_user_info(u):\n",
    "\n",
    "    user_dict = {\n",
    "        \"user_id\": u.get_id()[\"number\"], \n",
    "        \"first_name\": u.get_first_name(), \n",
    "        \"last_name\": u.get_last_name(), \n",
    "        \"state\": u.get_state(),\n",
    "        \"zip\": u.get_zipcode()\n",
    "    }\n",
    "    return user_dict\n",
    "\n",
    "get_user_info(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e9cd1a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<randomuser.RandomUser at 0x7f19dd6ac9d0>,\n",
       " <randomuser.RandomUser at 0x7f19ddd3c3a0>,\n",
       " <randomuser.RandomUser at 0x7f19dd759850>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_users = RandomUser.generate_users(250, {\"nat\": \"us\"})\n",
    "my_users[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fc4ca61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '606-72-0736',\n",
       "  'first_name': 'Richard',\n",
       "  'last_name': 'Morales',\n",
       "  'state': 'Indiana',\n",
       "  'zip': 73780},\n",
       " {'user_id': '314-10-6439',\n",
       "  'first_name': 'Brandy',\n",
       "  'last_name': 'Frazier',\n",
       "  'state': 'Massachusetts',\n",
       "  'zip': 98770},\n",
       " {'user_id': '750-17-4776',\n",
       "  'first_name': 'Jar',\n",
       "  'last_name': 'Snyder',\n",
       "  'state': 'Delaware',\n",
       "  'zip': 23469}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of 10 random users\n",
    "\n",
    "user_dicts = list(map(get_user_info, my_users))\n",
    "user_dicts[0:3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "96121a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'user_id': '000-35-7329',\n",
       "  'first_name': 'Cherly',\n",
       "  'last_name': 'Hale',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 58952},\n",
       " {'user_id': '719-30-1515',\n",
       "  'first_name': 'Tiffany',\n",
       "  'last_name': 'Henderson',\n",
       "  'state': 'Georgia',\n",
       "  'zip': 25120},\n",
       " {'user_id': '510-30-7858',\n",
       "  'first_name': 'Heather',\n",
       "  'last_name': 'Fernandez',\n",
       "  'state': 'Oregon',\n",
       "  'zip': 64756}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rdd = sc.parallelize(user_dicts)\n",
    "print(users_rdd.count())\n",
    "users_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "628f1734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': '219-42-2936',\n",
       "  'first_name': 'Clara',\n",
       "  'last_name': 'Hanson',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 54726},\n",
       " {'user_id': '477-31-3680',\n",
       "  'first_name': 'Sue',\n",
       "  'last_name': 'Ramirez',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 99043},\n",
       " {'user_id': '662-16-2597',\n",
       "  'first_name': 'Terra',\n",
       "  'last_name': 'Carpenter',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 42923},\n",
       " {'user_id': '963-57-9165',\n",
       "  'first_name': 'Sofia',\n",
       "  'last_name': 'Ryan',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 29804},\n",
       " {'user_id': '878-39-7497',\n",
       "  'first_name': 'Derek',\n",
       "  'last_name': 'Coleman',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 60498},\n",
       " {'user_id': '010-83-2249',\n",
       "  'first_name': 'Pauline',\n",
       "  'last_name': 'Martinez',\n",
       "  'state': 'Nebraska',\n",
       "  'zip': 32048},\n",
       " {'user_id': '000-35-7329',\n",
       "  'first_name': 'Cherly',\n",
       "  'last_name': 'Hale',\n",
       "  'state': 'Idaho',\n",
       "  'zip': 58952},\n",
       " {'user_id': '353-00-5562',\n",
       "  'first_name': 'Tamara',\n",
       "  'last_name': 'Spencer',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 46969},\n",
       " {'user_id': '468-63-1606',\n",
       "  'first_name': 'Louis',\n",
       "  'last_name': 'Gonzales',\n",
       "  'state': 'Hawaii',\n",
       "  'zip': 10712}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seelct_users_rdd = users_rdd.filter(lambda x: x['state'] in [\"Nebraska\", \"Hawaii\", \"Idaho\"])\n",
    "seelct_users_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef4a3e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building an RDD from a text file.\n",
    "text = sc.textFile('data/pride_and_prejudice.txt', minPartitions=4)\n",
    "### Number of items in the RDD\n",
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f176316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14579\n",
      "14579\n"
     ]
    }
   ],
   "source": [
    "print(text.count())\n",
    "\n",
    "print(len(open(\"data/pride_and_prejudice.txt\").readlines()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "81655fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen', '', 'This eBook is for the use of anyone anywhere in the United States and', 'most other parts of the world at no cost and with almost no restrictions', 'whatsoever. You may copy it, give it away or re-use it under the terms', 'of the Project Gutenberg License included with this eBook or online at', 'www.gutenberg.org. If you are not located in the United States, you', 'will have to check the laws of the country where you are located before', 'using this eBook.', '']\n"
     ]
    }
   ],
   "source": [
    "print(text.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b7b2eba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " 'OF',\n",
       " 'PRIDE',\n",
       " 'AND',\n",
       " 'PREJUDICE',\n",
       " 'BY',\n",
       " 'JANE',\n",
       " 'AUSTEN',\n",
       " 'THIS',\n",
       " 'EBOOK',\n",
       " 'IS',\n",
       " 'FOR',\n",
       " 'THE',\n",
       " 'USE',\n",
       " 'OF',\n",
       " 'ANYONE',\n",
       " 'ANYWHERE',\n",
       " 'IN',\n",
       " 'THE',\n",
       " 'UNITED',\n",
       " 'STATES',\n",
       " 'AND',\n",
       " 'MOST',\n",
       " 'OTHER',\n",
       " 'PARTS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'WORLD',\n",
       " 'AT',\n",
       " 'NO',\n",
       " 'COST',\n",
       " 'AND',\n",
       " 'WITH',\n",
       " 'ALMOST',\n",
       " 'NO',\n",
       " 'RESTRICTIONS',\n",
       " 'WHATSOEVER',\n",
       " 'YOU',\n",
       " 'MAY',\n",
       " 'COPY',\n",
       " 'IT',\n",
       " 'GIVE',\n",
       " 'IT',\n",
       " 'AWAY',\n",
       " 'OR',\n",
       " 'RE',\n",
       " 'USE',\n",
       " 'IT',\n",
       " 'UNDER',\n",
       " 'THE',\n",
       " 'TERMS',\n",
       " 'OF',\n",
       " 'THE',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'LICENSE',\n",
       " 'INCLUDED']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.flatMap(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19ce8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 1),\n",
       " ('PROJECT', 1),\n",
       " ('GUTENBERG', 1),\n",
       " ('EBOOK', 1),\n",
       " ('OF', 1),\n",
       " ('PRIDE', 1),\n",
       " ('AND', 1),\n",
       " ('PREJUDICE', 1),\n",
       " ('BY', 1),\n",
       " ('JANE', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to do something like the following\n",
    "# words_mapped = words.map(lambda x: (x,1))\n",
    "\n",
    "words_mapped = words.map(lambda x: (x,1))\n",
    "words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d160702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 1),\n",
       " ('PROJECT', 1),\n",
       " ('GUTENBERG', 1),\n",
       " ('EBOOK', 1),\n",
       " ('OF', 1),\n",
       " ('PRIDE', 1),\n",
       " ('AND', 1),\n",
       " ('PREJUDICE', 1),\n",
       " ('BY', 1),\n",
       " ('JANE', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d0107be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTH', 1),\n",
       " ('BOTTLE', 1),\n",
       " ('BOTTLE', 1),\n",
       " ('BOTTOM', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUGHT', 1),\n",
       " ('BOUND', 1),\n",
       " ('BOUND', 1),\n",
       " ('BOUND', 1),\n",
       " ('BOUNDARY', 1),\n",
       " ('BOUNDARY', 1),\n",
       " ('BOUNDLESS', 1),\n",
       " ('BOUNDS', 1),\n",
       " ('BOUNTY', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1),\n",
       " ('BOURGH', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_map.take(20000)[18000:18080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80a0a9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "print(add(1, 2))\n",
    "print(add(189, 11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62131881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223),\n",
       " ('NONSENSE', 8),\n",
       " ('THEREFORE', 75),\n",
       " ('VISIT', 53),\n",
       " ('PERHAPS', 76),\n",
       " ('PARTY', 58),\n",
       " ('THAN', 285),\n",
       " ('CONSIDER', 33),\n",
       " ('YOUR', 446),\n",
       " ('ESTABLISHMENT', 6),\n",
       " ('WILLIAM', 46),\n",
       " ('LUCAS', 70),\n",
       " ('DETERMINED', 32),\n",
       " ('THEY', 599),\n",
       " ('FEW', 72),\n",
       " ('HEARTY', 2),\n",
       " ('CONSENT', 13),\n",
       " ('DESIRE', 23),\n",
       " ('BIT', 3),\n",
       " ('OTHERS', 55),\n",
       " ('ALWAYS', 119),\n",
       " ('RECOMMEND', 12),\n",
       " ('SISTERS', 76),\n",
       " ('CHILDREN', 25),\n",
       " ('VEXING', 1),\n",
       " ('POOR', 38),\n",
       " ('NERVES', 4),\n",
       " ('HIGH', 17),\n",
       " ('OLD', 15),\n",
       " ('YEARS', 36),\n",
       " ('SINCE', 59),\n",
       " ('CAPRICE', 4),\n",
       " ('INSUFFICIENT', 10),\n",
       " ('UNDERSTANDING', 21),\n",
       " ('FANCIED', 7),\n",
       " ('NERVOUS', 4),\n",
       " ('VISITING', 5),\n",
       " ('WAITED', 7),\n",
       " ('ASSURING', 4),\n",
       " ('TILL', 93),\n",
       " ('EVENING', 74),\n",
       " ('AFTER', 200),\n",
       " ('FOLLOWING', 27),\n",
       " ('TRIMMING', 1),\n",
       " ('SUDDENLY', 16),\n",
       " ('ADDRESSED', 17),\n",
       " ('WE', 255),\n",
       " ('FORGET', 17),\n",
       " ('SHALL', 164),\n",
       " ('PROMISED', 21),\n",
       " ('TWO', 131)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = words_mapped.reduceByKey(add)\n",
    "counts.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee3eca",
   "metadata": {},
   "source": [
    "Since in fucntional programing, we need to always returns new data instead of manipulating the data in-place we can re-write the above cleanly using:\n",
    "    \n",
    "```\n",
    "\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(add)\n",
    "counts_test_2.take(100)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd67ef09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 52),\n",
       " ('UNITED', 22),\n",
       " ('OTHER', 227),\n",
       " ('WORLD', 68),\n",
       " ('NO', 501),\n",
       " ('GIVE', 127),\n",
       " ('LICENSE', 18),\n",
       " ('WWW', 9),\n",
       " ('ARE', 361),\n",
       " ('TO', 4245),\n",
       " ('DATE', 5),\n",
       " ('UPDATED', 2),\n",
       " ('ENGLISH', 1),\n",
       " ('CHARACTER', 65),\n",
       " ('PRODUCED', 13),\n",
       " ('ILLUSTRATED', 1),\n",
       " ('THAT', 1555),\n",
       " ('POSSESSION', 10),\n",
       " ('LITTLE', 187),\n",
       " ('KNOWN', 58),\n",
       " ('VIEWS', 11),\n",
       " ('CONSIDERED', 23),\n",
       " ('AS', 1193),\n",
       " ('ONE', 273),\n",
       " ('THEIR', 439),\n",
       " ('MR', 784),\n",
       " ('JUST', 72),\n",
       " ('TOLD', 69),\n",
       " ('ME', 427),\n",
       " ('ANSWER', 65),\n",
       " ('WHO', 288),\n",
       " ('TELL', 71),\n",
       " ('HEARING', 24),\n",
       " ('ENOUGH', 106),\n",
       " ('WHY', 53),\n",
       " ('YOUNG', 130),\n",
       " ('MONDAY', 8),\n",
       " ('FOUR', 35),\n",
       " ('MUCH', 327),\n",
       " ('AGREED', 13),\n",
       " ('MICHAELMAS', 2),\n",
       " ('SERVANTS', 13),\n",
       " ('WEEK', 29),\n",
       " ('NAME', 34),\n",
       " ('BINGLEY', 307),\n",
       " ('OH', 96),\n",
       " ('FIVE', 32),\n",
       " ('YEAR', 29),\n",
       " ('FINE', 31),\n",
       " ('CAN', 223),\n",
       " ('NONSENSE', 8),\n",
       " ('THEREFORE', 75),\n",
       " ('VISIT', 53),\n",
       " ('PERHAPS', 76),\n",
       " ('PARTY', 58),\n",
       " ('THAN', 285),\n",
       " ('CONSIDER', 33),\n",
       " ('YOUR', 446),\n",
       " ('ESTABLISHMENT', 6),\n",
       " ('WILLIAM', 46),\n",
       " ('LUCAS', 70),\n",
       " ('DETERMINED', 32),\n",
       " ('THEY', 599),\n",
       " ('FEW', 72),\n",
       " ('HEARTY', 2),\n",
       " ('CONSENT', 13),\n",
       " ('DESIRE', 23),\n",
       " ('BIT', 3),\n",
       " ('OTHERS', 55),\n",
       " ('ALWAYS', 119),\n",
       " ('RECOMMEND', 12),\n",
       " ('SISTERS', 76),\n",
       " ('CHILDREN', 25),\n",
       " ('VEXING', 1),\n",
       " ('POOR', 38),\n",
       " ('NERVES', 4),\n",
       " ('HIGH', 17),\n",
       " ('OLD', 15),\n",
       " ('YEARS', 36),\n",
       " ('SINCE', 59),\n",
       " ('CAPRICE', 4),\n",
       " ('INSUFFICIENT', 10),\n",
       " ('UNDERSTANDING', 21),\n",
       " ('FANCIED', 7),\n",
       " ('NERVOUS', 4),\n",
       " ('VISITING', 5),\n",
       " ('WAITED', 7),\n",
       " ('ASSURING', 4),\n",
       " ('TILL', 93),\n",
       " ('EVENING', 74),\n",
       " ('AFTER', 200),\n",
       " ('FOLLOWING', 27),\n",
       " ('TRIMMING', 1),\n",
       " ('SUDDENLY', 16),\n",
       " ('ADDRESSED', 17),\n",
       " ('WE', 255),\n",
       " ('FORGET', 17),\n",
       " ('SHALL', 164),\n",
       " ('PROMISED', 21),\n",
       " ('TWO', 131)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(add)\n",
    "counts_test_2.take(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618d9dd",
   "metadata": {},
   "source": [
    "### Spark and Lazy Evaluation\n",
    "\n",
    "\n",
    "* Transformation on an RDD  are delayed until an action is performed\n",
    "\n",
    "  * Similar to python genertor\n",
    "\n",
    "  * This is called lazy evaluation \n",
    "\n",
    "* You can chain many transformations on the same RDD without causing any execution. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4a9b177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aeac4d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[133] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following wo't return an error until an action is performed\n",
    "text.map(lambda x: len(x)/0).filter(lambda x: x>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1a17040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/20 18:49:38 ERROR Executor: Exception in task 0.0 in stage 68.0 (TID 179)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_33/2990878127.py\", line 1, in <lambda>\n",
      "TypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/09/20 18:49:38 WARN TaskSetManager: Lost task 0.0 in stage 68.0 (TID 179) (4eedec98e4d1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_33/2990878127.py\", line 1, in <lambda>\n",
      "TypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/09/20 18:49:38 ERROR TaskSetManager: Task 0 in stage 68.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 179) (4eedec98e4d1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_33/2990878127.py\", line 1, in <lambda>\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_33/2990878127.py\", line 1, in <lambda>\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33/2416491165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The following will generate an error since the transformation dividing by 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# is executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 179) (4eedec98e4d1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_33/2990878127.py\", line 1, in <lambda>\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_33/2990878127.py\", line 1, in <lambda>\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# The following will generate an error since the transformation dividing by 0 \n",
    "# is executed\n",
    "# the `ZeroDivisionError: division by zero` is burried in many Scala error messages.\n",
    "\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c483f56",
   "metadata": {},
   "source": [
    "### The Spark Computation DAG\n",
    "\n",
    "* Lazy evaluation is possible because Spark maintains a graph (DAG) of the transformation transformations\n",
    "* The transformating are optimized and executed in the graph once an action is triggered\n",
    "\n",
    "* A simple exampe of an execution is:\n",
    "\n",
    "```python\n",
    "data_2 =  data_1.map(lambda x: x+2)\n",
    "# do some work here\n",
    "data_3 =  data_2.map(lambda x: x-2)\n",
    "```\n",
    "* The above transformations are not run because it does not change the value of `x`.\n",
    "  * `data_3` is equal to `data_1`\n",
    "\n",
    "* See the the following blog post about the catalyst optimizer.\n",
    "\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
