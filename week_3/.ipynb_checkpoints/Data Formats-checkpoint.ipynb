{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449f7aab",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This tutorial is designed to accomplish following learning objectives\n",
    "\n",
    "* Some of the popular data formats\n",
    "     * columnar and Row wise formatting of data,\n",
    "     * how different data formats affects wrangling of big data\n",
    "     * Pros and cons of different file formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed420e46",
   "metadata": {},
   "source": [
    "File Format: a Quick intuition\n",
    "\n",
    "* In big data, the right storage format  is paramount for achiving perfomance, saving space and makign certain operations possible.\n",
    "* Can save time, cost, improve computation time etc.\n",
    "\n",
    "* We're accustomed to row-based formats\n",
    "  * Excel file like file where each row is an table entry\n",
    "| Transaction Date \t| Nb Items \t| Total   \t|\n",
    "|------------------\t|----------\t|---------\t|\n",
    "| 01/01/2001       \t| 4        \t| 1852.14 \t|\n",
    "| 01/01/2001       \t| 3        \t| 968.00  \t|\n",
    "| `...`             | `...`     | `...`     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8017d4a",
   "metadata": {},
   "source": [
    "File Format: a Quick intuition - cont'd \n",
    "    \n",
    "* This format may be inappropriate for certain types of data or operations\n",
    "\n",
    "* Data: Imagine, that sales info above contians hundreds of millions of transactions with hundreds of thousands of transactions per day\n",
    "    * The same transaction dates will be unnecessarily duplicated hundrend of thousands of time.\n",
    "    * Perhaps a dictionari like format where the key is the date.\n",
    " \n",
    "```python\n",
    "# notice the two ellipses.\n",
    "{\"01/01/2001\": ((4, 1852.14), (3,  968.00), ...), ...}\n",
    "```\n",
    "\n",
    "* Operation: Imagine that objectives if to compute the total sales\n",
    "  * We need to read millions of lines to compute a single values.\n",
    "  * Perhaps we can store the data as row data. Reading a single line is sufficient to compute the average.\n",
    "\n",
    "|              |      |  |   |\n",
    "| :---              |    :----:  | :--------: |:---:|\n",
    "| **Totals** \t        | 1852.14    | 968.00     | `...` |\n",
    "| **Transaction Dates** | 01/01/2001 | 01/01/2001 | `...` |\n",
    "| **Items**       \t    | 4        \t | 3\t      | `...` |\n",
    "\n",
    "\n",
    "* Question: Can you think of a scenario where the data format above is not ideal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### File Formats\n",
    "\n",
    "* There 4 consideration when selecting file fomats:\n",
    "    * Row vs Column\n",
    "    * Schema Management\n",
    "    * Spilitability\n",
    "    * Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2bb0e",
   "metadata": {},
   "source": [
    "### 1- Row- and Column-Based Formats\n",
    "\n",
    "\n",
    "* An importan consideration when selecting a big data format\n",
    "\n",
    "* Row-based: Ideal when using all the data \n",
    "  * example, building a machine learning models that requires all the features\n",
    "  * if not all but a subset, unecessary data can be removed\n",
    "    * Can drop data after each read for large datasets.\n",
    "      * Avoid loading complete dataset in RAM\n",
    "\n",
    "\n",
    "* Column-based storage: useful when performing operation on a subset of columns\n",
    "  * Computing total sales, Or Total aggregated by date, etc.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b5853",
   "metadata": {},
   "source": [
    "### Row-Based Formats\n",
    "\n",
    "* Simplest form of data \n",
    "* Used in many applications, from web log files to highly-structured database systems like MySql and Oracle.\n",
    "\n",
    "* A row is a a new instance,i.e., since object containing values for all the variables (or features).\n",
    "\n",
    "* processing the data would require reading all inputs line by line\n",
    "\n",
    "\n",
    "* This is commonly used for Online Transactional Processing (OLTP). \n",
    "  * OLTP systems usually process CRUD queries (Create, Read, Update and Delete) at a record level.\n",
    "  * The main emphasis for OLTP systems is focus on maintaining data integrity in multi-access environments and an effectiveness measured by number of transactions per second\n",
    "   * More on this when we dicuss big data platforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e001a",
   "metadata": {},
   "source": [
    "### Column Based Formats\n",
    "\n",
    "\n",
    "* Data is grouped by columns\n",
    "* Easy to focus computation on specific columns of data\n",
    "  * Ex. Search for larger value is easier since data is stored sequentially by column. \n",
    "  \n",
    "* Ideal for compression\n",
    "  * compression codecs (ex. GZIP) have a higher compression-ratio when compressing sequences of similar data. \n",
    "  * Let's do an experiment\n",
    "    * We'll do such small experiemtn a lot, to get a handle on Python.\n",
    "    \n",
    "  * Typically, the slowest component in large distribution system are the disk and network\n",
    "      * Using compression reduces read IO and transfers, thus speeding up the analysis.\n",
    "    \n",
    "* This way of processing data is usually called OLAP (OnLine Analytical Processing) query.    \n",
    " * OLAP is an approach designed to quickly answer analytics queries involving multiple dimensions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b956e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 4, 1, 1, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices([1,2,3,4], k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a583d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'C', 'T', 'T', 'A', 'G']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices(\"ACGT\", k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85e5d052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0b23c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zlib \n",
    "import string\n",
    "# let's randomly generate two string of 1000, an ASCII and an INT\n",
    "\n",
    "random_ASCII = random.choices(string.printable, k=1000)\n",
    "random_numbers = random.choices(string.digits, k=1000)\n",
    "\n",
    "len(zlib.compress( str.encode(\"\".join(random_ASCII))))\n",
    "len(zlib.compress( str.encode(\"\".join(random_numbers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279abc9",
   "metadata": {},
   "source": [
    "### Column-based formats: Advantages and Disadvantages\n",
    "\n",
    "Advantages: \n",
    "* Columnar-storage of data can yield sometimes 100x- 1000x performance improvement, specifically on data with hundreds of columns\n",
    "    * Those are called wide data\n",
    "\n",
    "Disadvantages:\n",
    "  * Hard to read by a human but how useful is a row-based columns anyway\n",
    "  * Can be more CPU intensive to write for very large data.\n",
    "    * Need to collect the data for each column before writing it file\n",
    "  * difficult to access a sinlge instnaces (entry across all values)\n",
    "  * Not efficient with CRUD queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a4a3e",
   "metadata": {},
   "source": [
    "### 2- Datatype & Schema Enforcement and Evolution \n",
    "\n",
    "* \"Schema\" in a database context, means structure and organization of the data  \n",
    "    * how you stucture the data: datatypes, missing values, primary keys, etc, indices, etc.\n",
    "    * Organization: relationships across tables.\n",
    "\n",
    "* Here, we mostly refer to the data type\n",
    "\n",
    "* Whith a simple text format, (e.g.: table with values sepeated by space), datatype cannot be declared or enforced\n",
    "\n",
    "* Declaring the type of a value provides some advantages.\n",
    "  * Storage requirements: String will require more storage than boolean (2 bytes)\n",
    "  * Data validity: guaranties that the dataset is valid\n",
    "  * Compression: We know how to compress different types.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c5229",
   "metadata": {},
   "source": [
    "### 2- Datatype & Schema Enforcement and Evolution  - Cont'd\n",
    "Unless your data is guaranteed to never change, you’ll need to think about schema evolution, or how your data schema changes over time. How will your file format manage fields that are added or deleted? When evaluating schema evolution specifically, there are a few key questions to ask of any data format:\n",
    "\n",
    "How easy is it to update a schema (such as adding a field, removing or renaming a field)?\n",
    "How will different versions of the schema “talk” to each other?\n",
    "Is it human-readable? Does it need to be?\n",
    "How fast can the schema be processed?\n",
    "How does it impact the size of data?\n",
    "\n",
    "Example, for a dictionary ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de7933",
   "metadata": {},
   "source": [
    "### 3- Splitability\n",
    "\n",
    "* Big data can often comprise many millions of recrods, often split across a large number of files\n",
    "  * Think of instance monthly logs, yearly transction, daily airplane sensorts\n",
    "* Often useful to split the data across multiple machine and execute each computation separately\n",
    "\n",
    "* Some files formats are more amenable to splitting than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3- Splitability - Row-based\n",
    "\n",
    "Row-based formats can be split along row boundaries\n",
    "\n",
    "```\n",
    "# file 1  with n lines\n",
    "01/01/2001       \t4        \t1852.14\n",
    "01/01/2001       \t3        \t968.00\n",
    "... \n",
    "```\n",
    "\n",
    "* Splitting `file 1` with `n` across `m` total machines is easy.\n",
    "  * `m-1` machines gets `round(n/m)` unique lines, last machine get remaining lines\n",
    "      * Machine 1 gets lines 1 though $\\frac{n}{m}$\n",
    "      * Machine 2 gets lines 1 though $\\frac{n}{m}+1$ to  $2 * \\frac{n}{m}$ \n",
    "      * machine x\n",
    "      \n",
    "      \n",
    "* challenges: \n",
    "    * Partitioninon over particular column value can be difficult if data stored in a random order. \n",
    "    * E.g.: splitting on number of items required sorting data on 2nd column first, then splitting after reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f3d5a",
   "metadata": {},
   "source": [
    "### 3- Splitability row-based, nested \n",
    "\n",
    "* Some files formats are more amenable to splitting than others.\n",
    "\n",
    "\n",
    "``` \n",
    "file 2\n",
    "{\"01/01/20014\": [(4, 1852.14), (3, 968.00)], ....}\n",
    "```\n",
    "\n",
    "* You cannot easily split this file this file format without parsing the file first.\n",
    "  * Need to read the compelte file to split it into chunks.\n",
    "    * Data may need to ne loaded in RAM first.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aad4d4",
   "metadata": {},
   "source": [
    "### 3- Splitability: Column-based, nested \n",
    "\n",
    "\n",
    "* A column-based format can be split if the comutation is column-specific.\n",
    "\n",
    "``` \n",
    "# file 3\n",
    "date: 01/01/2001, 01/01/2001\n",
    "nb_items: 4, 3\n",
    "totals: 1852.14, 968.00\n",
    "```\n",
    "\n",
    "* With the example above, each machine is concerned with a computation on a a specific variable. For example: \n",
    "  * Machine 1 takes `date` data and computes the number of sales per months\n",
    "  * Machine 2 takes the `nb_items` data and computes the number of sales per months\n",
    "  * Machine 3 takes the `totals` data and computes the total sales values\n",
    "  \n",
    "* Machine don't have any knowledge of variables that are not given. \n",
    "  * E.g. maachine three is not given date info and cannot compute, for example, the monthly of weekly sales average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0ff77",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* File in the CSV (Comma-serparated values) format are usually used to exchange tabular dat \n",
    "  * Plain-text file (readable characters)\n",
    "  \n",
    "* CSV is a row-based file format: each row of the file is a separate data instance\n",
    "  * May or may not contain a header\n",
    "* structure is conveyed throught explicit commas\n",
    "  * text commas are encapsulated in double quotes\n",
    "\n",
    "```\n",
    "Title,Author,Genre,Height,Publisher\n",
    "\"Computer Vision, A Modern Approach\",\"Forsyth, David\",data_science,255,Pearson\n",
    "Data Mining Handbook,\"Nisbet, Robert\",data_science,242,Apress\n",
    "Making Software,\"Oram, Andy\",computer_science,232,O'Reilly\n",
    "....\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae2028",
   "metadata": {},
   "source": [
    "* Data Strucure conveyed through redundant values across\n",
    "\n",
    "* Data connections are usually established using multiple CSV files. \n",
    "   * Uses foreign keys (specific columns) across files\n",
    "   * Connection not expressed in the file format \n",
    "  \n",
    "* CSV format is not fully standardized\n",
    "  * files may be sepated by other chatacters such as tabs (tsv) or spaces (ssv)\n",
    "  \n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
