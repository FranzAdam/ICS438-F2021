{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449f7aab",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This tutorial is designed to accomplish following learning objectives\n",
    "\n",
    "* Some of the popular data formats\n",
    "     * columnar and Row wise formatting of data,\n",
    "     * how different data formats affects wrangling of big data\n",
    "     * Pros and cons of different file formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed420e46",
   "metadata": {},
   "source": [
    "File Format: a Quick intuition\n",
    "\n",
    "* In big data, the right storage format  is paramount for achiving perfomance, saving space and makign certain operations possible.\n",
    "* Can save time, cost, improve computation time etc.\n",
    "\n",
    "* We're accustomed to row-based formats\n",
    "  * Excel file like file where each row is an table entry\n",
    "| Transaction Date \t| Nb Items \t| Total   \t|\n",
    "|------------------\t|----------\t|---------\t|\n",
    "| 01/01/2001       \t| 4        \t| 1852.14 \t|\n",
    "| 01/01/2001       \t| 3        \t| 968.00  \t|\n",
    "| `...`             | `...`     | `...`     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8017d4a",
   "metadata": {},
   "source": [
    "File Format: a Quick intuition - cont'd \n",
    "    \n",
    "* This format may be inappropriate for certain types of data or operations\n",
    "\n",
    "* Data: Imagine, that sales info above contians hundreds of millions of transactions with hundreds of thousands of transactions per day\n",
    "    * The same transaction dates will be unnecessarily duplicated hundrend of thousands of time.\n",
    "    * Perhaps a dictionari like format where the key is the date.\n",
    " \n",
    "```python\n",
    "# notice the two ellipses.\n",
    "{\"01/01/2001\": ((4, 1852.14), (3,  968.00), ...), ...}\n",
    "```\n",
    "\n",
    "* Operation: Imagine that objectives if to compute the total sales\n",
    "  * We need to read millions of lines to compute a single values.\n",
    "  * Perhaps we can store the data as row data. Reading a single line is sufficient to compute the average.\n",
    "\n",
    "|              |      |  |   |\n",
    "| :---              |    :----:  | :--------: |:---:|\n",
    "| **Totals** \t        | 1852.14    | 968.00     | `...` |\n",
    "| **Transaction Dates** | 01/01/2001 | 01/01/2001 | `...` |\n",
    "| **Items**       \t    | 4        \t | 3\t      | `...` |\n",
    "\n",
    "\n",
    "* Question: Can you think of a scenario where the data format above is not ideal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### File Formats\n",
    "\n",
    "* There 4 consideration when selecting file fomats:\n",
    "    * Row vs Column\n",
    "    * Schema Management\n",
    "    * Spilitability\n",
    "    * Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2bb0e",
   "metadata": {},
   "source": [
    "### 1- Row- and Column-Based Formats\n",
    "\n",
    "\n",
    "* An importan consideration when selecting a big data format\n",
    "\n",
    "* Row-based: Ideal when using all the data \n",
    "  * example, building a machine learning models that requires all the features\n",
    "  * if not all but a subset, unecessary data can be removed\n",
    "    * Can drop data after each read for large datasets.\n",
    "      * Avoid loading complete dataset in RAM\n",
    "\n",
    "\n",
    "* Column-based storage: useful when performing operation on a subset of columns\n",
    "  * Computing total sales, Or Total aggregated by date, etc.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b5853",
   "metadata": {},
   "source": [
    "### Row-Based Formats\n",
    "\n",
    "* Simplest form of data \n",
    "* Used in many applications, from web log files to highly-structured database systems like MySql and Oracle.\n",
    "\n",
    "* A row is a a new instance,i.e., since object containing values for all the variables (or features).\n",
    "\n",
    "* processing the data would require reading all inputs line by line\n",
    "\n",
    "\n",
    "* This is commonly used for Online Transactional Processing (OLTP). \n",
    "  * OLTP systems usually process CRUD queries (Create, Read, Update and Delete) at a record level.\n",
    "  * The main emphasis for OLTP systems is focus on maintaining data integrity in multi-access environments and an effectiveness measured by number of transactions per second\n",
    "   * More on this when we dicuss big data platforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e001a",
   "metadata": {},
   "source": [
    "### Column Based Formats\n",
    "\n",
    "\n",
    "* Data is grouped by columns\n",
    "* Easy to focus computation on specific columns of data\n",
    "  * Ex. Search for larger value is easier since data is stored sequentially by column. \n",
    "  \n",
    "* Ideal for compression\n",
    "  * compression codecs (ex. GZIP) have a higher compression-ratio when compressing sequences of similar data. \n",
    "  * Let's do an experiment\n",
    "    * We'll do such small experiemtn a lot, to get a handle on Python.\n",
    "    \n",
    "  * Typically, the slowest component in large distribution system are the disk and network\n",
    "      * Using compression reduces read IO and transfers, thus speeding up the analysis.\n",
    "    \n",
    "* This way of processing data is usually called OLAP (OnLine Analytical Processing) query.    \n",
    " * OLAP is an approach designed to quickly answer analytics queries involving multiple dimensions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b956e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 4, 1, 1, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices([1,2,3,4], k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a583d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'C', 'T', 'T', 'A', 'G']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices(\"ACGT\", k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85e5d052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0b23c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zlib \n",
    "import string\n",
    "# let's randomly generate two string of 1000, an ASCII and an INT\n",
    "\n",
    "random_ASCII = random.choices(string.printable, k=1000)\n",
    "random_numbers = random.choices(string.digits, k=1000)\n",
    "\n",
    "len(zlib.compress( str.encode(\"\".join(random_ASCII))))\n",
    "len(zlib.compress( str.encode(\"\".join(random_numbers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279abc9",
   "metadata": {},
   "source": [
    "### Column-based formats: Advantages and Disadvantages\n",
    "\n",
    "Advantages: \n",
    "* Columnar-storage of data can yield sometimes 100x- 1000x performance improvement, specifically on data with hundreds of columns\n",
    "    * Those are called wide data\n",
    "\n",
    "Disadvantages:\n",
    "  * Hard to read by a human but how useful is a row-based columns anyway\n",
    "  * Can be more CPU intensive to write for very large data.\n",
    "    * Need to collect the data for each column before writing it file\n",
    "  * difficult to access a sinlge instnaces (entry across all values)\n",
    "  * Not efficient with CRUD queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a4a3e",
   "metadata": {},
   "source": [
    "### 2- Datatype & Schema Enforcement and Evolution \n",
    "\n",
    "* \"Schema\" in a database context, means structure and organization of the data  \n",
    "    * how you stucture the data: datatypes, missing values, primary keys, etc, indices, etc.\n",
    "    * Organization: relationships across tables.\n",
    "\n",
    "* Here, we mostly refer to the data type\n",
    "\n",
    "* Whith a simple text format, (e.g.: table with values sepeated by space), datatype cannot be declared or enforced\n",
    "\n",
    "* Declaring the type of a value provides some advantages.\n",
    "  * Storage requirements: String will require more storage than boolean (2 bytes)\n",
    "  * Data validity: guaranties that the dataset is valid\n",
    "  * Compression: We know how to compress different types.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c5229",
   "metadata": {},
   "source": [
    "### 2- Datatype & Schema Enforcement and Evolution  - Cont'd\n",
    "Unless your data is guaranteed to never change, you’ll need to think about schema evolution, or how your data schema changes over time. How will your file format manage fields that are added or deleted? When evaluating schema evolution specifically, there are a few key questions to ask of any data format:\n",
    "\n",
    "How easy is it to update a schema (such as adding a field, removing or renaming a field)?\n",
    "How will different versions of the schema “talk” to each other?\n",
    "Is it human-readable? Does it need to be?\n",
    "How fast can the schema be processed?\n",
    "How does it impact the size of data?\n",
    "\n",
    "Example, for a dictionary ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de7933",
   "metadata": {},
   "source": [
    "### 3- Splitability\n",
    "\n",
    "* Big data can often comprise many millions of recrods, often split across a large number of files\n",
    "  * Think of instance monthly logs, yearly transction, daily airplane sensorts\n",
    "* Often useful to split the data across multiple machine and execute each computation separately\n",
    "\n",
    "* Some files formats are more amenable to splitting than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3- Splitability - Row-based\n",
    "\n",
    "Row-based formats can be split along row boundaries\n",
    "\n",
    "```\n",
    "# file 1  with n lines\n",
    "01/01/2001       \t4        \t1852.14\n",
    "01/01/2001       \t3        \t968.00\n",
    "... \n",
    "```\n",
    "\n",
    "* Splitting `file 1` with `n` across `m` total machines is easy.\n",
    "  * `m-1` machines gets `round(n/m)` unique lines, last machine get remaining lines\n",
    "      * Machine 1 gets lines 1 though $\\frac{n}{m}$\n",
    "      * Machine 2 gets lines 1 though $\\frac{n}{m}+1$ to  $2 * \\frac{n}{m}$ \n",
    "      * machine x\n",
    "      \n",
    "      \n",
    "* challenges: \n",
    "    * Partitioninon over particular column value can be difficult if data stored in a random order. \n",
    "    * E.g.: splitting on number of items required sorting data on 2nd column first, then splitting after reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f3d5a",
   "metadata": {},
   "source": [
    "### 3- Splitability row-based, nested \n",
    "\n",
    "* Some files formats are more amenable to splitting than others.\n",
    "\n",
    "\n",
    "``` \n",
    "file 2\n",
    "{\"01/01/20014\": [(4, 1852.14), (3, 968.00)], ....}\n",
    "```\n",
    "\n",
    "* You cannot easily split this file this file format without parsing the file first.\n",
    "  * Need to read the compelte file to split it into chunks.\n",
    "    * Data may need to ne loaded in RAM first.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aad4d4",
   "metadata": {},
   "source": [
    "### 3- Splitability: Column-based, nested \n",
    "\n",
    "\n",
    "* A column-based format can be split if the comutation is column-specific.\n",
    "\n",
    "``` \n",
    "# file 3\n",
    "date: 01/01/2001, 01/01/2001\n",
    "nb_items: 4, 3\n",
    "totals: 1852.14, 968.00\n",
    "```\n",
    "\n",
    "* With the example above, each machine is concerned with a computation on a a specific variable. For example: \n",
    "  * Machine 1 takes `date` data and computes the number of sales per months\n",
    "  * Machine 2 takes the `nb_items` data and computes the number of sales per months\n",
    "  * Machine 3 takes the `totals` data and computes the total sales values\n",
    "  \n",
    "* Machine don't have any knowledge of variables that are not given. \n",
    "  * E.g. maachine three is not given date info and cannot compute, for example, the monthly of weekly sales average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3a8c1",
   "metadata": {},
   "source": [
    "### 4- Compression\n",
    "\n",
    "\n",
    "* When working on a distributed system, data trnsfers can be a serious bottleneck\n",
    "* Compression can substantially improve runtime and storage requirements\n",
    "\n",
    "* Columnar data can achieve better compression rates than row-based data \n",
    "  * Simple way to think about it: column will have a lot more duplicate values:\n",
    "      * Ex. Age Column: 21,22,21,24,25,21,22,21,19,21,21,22, ....\n",
    "      \n",
    "* Compression complex compression algorithms on very large files ca save on space but substantially increase compute time.\n",
    "    * Uncomression/re-compression needs to occr everytime you need to access the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6279c",
   "metadata": {},
   "source": [
    "### Standardization and File Formats\n",
    "'\n",
    "* Naturaully, one can adopt to structure a file (either binary or txt) in their own format\n",
    "  * Many companies may choose to do so internally for many reasons\n",
    "  * E.g.: \n",
    "\n",
    "```\n",
    "RECORD 1: \n",
    "FIRST_NAME_1\\sLAST_NAME_1\\tFIRST_NAME_2\\sLAST_NAME_1\\tFIRST_NAME_2\\sLAST_NAME_1...\n",
    "POSTION_1\\sSALARY_1\\tPOSTION_1\\sSALARY_2\\tPOSTION_1\\sSALARY_3\n",
    "```\n",
    "\n",
    "* However, there are many benefits to using a standard file format. E.g.:\n",
    "  * clarity — eliminating the need for guesswork or extra searching\n",
    "  * Quality — designed by large teams and used extensively, which provides opportunities to find and correct bugs\n",
    "  * Productivity — no need to maintain internal doc. easier to get answer online when issues arise.\n",
    "  * Interoperability - you data is no longer locked to your company. Can be used acros platforms\n",
    "\n",
    "* Some of the most used formats are CSV, JSON, Parquet, AVRO HDF5\n",
    "  * Very well supported in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0ff77",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* File in the CSV (Comma-serparated values) format are usually used to exchange tabular dat \n",
    "  * Plain-text file (readable characters)\n",
    "  \n",
    "* CSV is a row-based file format: each row of the file is a separate data instance\n",
    "  * May or may not contain a header\n",
    "* structure is conveyed throught explicit commas\n",
    "  * text commas are encapsulated in double quotes\n",
    "\n",
    "```\n",
    "Title,Author,Genre,Height,Publisher\n",
    "\"Computer Vision, A Modern Approach\",\"Forsyth, David\",data_science,255,Pearson\n",
    "Data Mining Handbook,\"Nisbet, Robert\",data_science,242,Apress\n",
    "Making Software,\"Oram, Andy\",computer_science,232,O'Reilly\n",
    "....\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae2028",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* CSV format is not fully standardized\n",
    "  * files may be sepated by other chatacters such as tabs (tsv) or spaces (ssv)\n",
    "  \n",
    "* Data connections are usually established using multiple CSV files. \n",
    "   * Uses foreign keys (specific columns) across files\n",
    "   * Connection not expressed in the file format \n",
    "  \n",
    "* Data Strucure conveyed through redundant values across files\n",
    "\n",
    "* Native support in Python\n",
    "```python\n",
    "import csv\n",
    "# use csv ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e31127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: ['Rank', 'Year', 'Movie', 'WorldwideBox Office', 'DomesticBox Office', 'InternationalBox Office']\n",
      "Line 1: ['1', '2009', 'Avatar', '$2,845,899,541', '$760,507,625', '$2,085,391,916']\n",
      "Line 2: ['2', '2019', 'Avengers: Endgame', '$2,797,800,564', '$858,373,000', '$1,939,427,564']\n",
      "Line 3: ['3', '1997', 'Titanic', '$2,207,986,545', '$659,363,944', '$1,548,622,601']\n",
      "Line 4: ['4', '2015', 'Star Wars Ep. VII: The Force Awakens', '$2,064,615,817', '$936,662,225', '$1,127,953,592']\n",
      "Line 5: ['5', '2018', 'Avengers: Infinity War', '$2,044,540,523', '$678,815,482', '$1,365,725,041']\n",
      "Line 6: ['6', '2015', 'Jurassic World', '$1,669,979,967', '$652,306,625', '$1,017,673,342']\n",
      "Line 7: ['7', '2019', 'The Lion King', '$1,654,367,425', '$543,638,043', '$1,110,729,382']\n",
      "Line 8: ['8', '2015', 'Furious 7', '$1,516,881,526', '$353,007,020', '$1,163,874,506']\n",
      "Line 9: ['9', '2012', 'The Avengers', '$1,515,100,211', '$623,357,910', '$891,742,301']\n",
      "Line 10: ['10', '2019', 'Frozen II', '$1,446,925,396', '$477,373,578', '$969,551,818']\n"
     ]
    }
   ],
   "source": [
    "# All_Time_Worldwide_Box_Office_partial.csv\n",
    "import csv\n",
    "csvfile = open('../Data/All_Time_Worldwide_Box_Office_partial.csv') \n",
    "movies_file = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "i = 0 \n",
    "for line in movies_file:\n",
    "    print(f\"Line {i}: {line}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a5b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: {'Rank': '1', 'Year': '2009', 'Movie': 'Avatar', 'WorldwideBox Office': '$2,845,899,541', 'DomesticBox Office': '$760,507,625', 'InternationalBox Office': '$2,085,391,916'}\n",
      "Line 1: {'Rank': '2', 'Year': '2019', 'Movie': 'Avengers: Endgame', 'WorldwideBox Office': '$2,797,800,564', 'DomesticBox Office': '$858,373,000', 'InternationalBox Office': '$1,939,427,564'}\n",
      "Line 2: {'Rank': '3', 'Year': '1997', 'Movie': 'Titanic', 'WorldwideBox Office': '$2,207,986,545', 'DomesticBox Office': '$659,363,944', 'InternationalBox Office': '$1,548,622,601'}\n",
      "Line 3: {'Rank': '4', 'Year': '2015', 'Movie': 'Star Wars Ep. VII: The Force Awakens', 'WorldwideBox Office': '$2,064,615,817', 'DomesticBox Office': '$936,662,225', 'InternationalBox Office': '$1,127,953,592'}\n",
      "Line 4: {'Rank': '5', 'Year': '2018', 'Movie': 'Avengers: Infinity War', 'WorldwideBox Office': '$2,044,540,523', 'DomesticBox Office': '$678,815,482', 'InternationalBox Office': '$1,365,725,041'}\n",
      "Line 5: {'Rank': '6', 'Year': '2015', 'Movie': 'Jurassic World', 'WorldwideBox Office': '$1,669,979,967', 'DomesticBox Office': '$652,306,625', 'InternationalBox Office': '$1,017,673,342'}\n",
      "Line 6: {'Rank': '7', 'Year': '2019', 'Movie': 'The Lion King', 'WorldwideBox Office': '$1,654,367,425', 'DomesticBox Office': '$543,638,043', 'InternationalBox Office': '$1,110,729,382'}\n",
      "Line 7: {'Rank': '8', 'Year': '2015', 'Movie': 'Furious 7', 'WorldwideBox Office': '$1,516,881,526', 'DomesticBox Office': '$353,007,020', 'InternationalBox Office': '$1,163,874,506'}\n",
      "Line 8: {'Rank': '9', 'Year': '2012', 'Movie': 'The Avengers', 'WorldwideBox Office': '$1,515,100,211', 'DomesticBox Office': '$623,357,910', 'InternationalBox Office': '$891,742,301'}\n",
      "Line 9: {'Rank': '10', 'Year': '2019', 'Movie': 'Frozen II', 'WorldwideBox Office': '$1,446,925,396', 'DomesticBox Office': '$477,373,578', 'InternationalBox Office': '$969,551,818'}\n"
     ]
    }
   ],
   "source": [
    "# All_Time_Worldwide_Box_Office_partial.csv\n",
    "import csv\n",
    "csvfile = open('../Data/All_Time_Worldwide_Box_Office_partial.csv') \n",
    "movies_file = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "i = 0 \n",
    "for line in movies_file:\n",
    "    print(f\"Line {i}: {line}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a748ae3",
   "metadata": {},
   "source": [
    "### CSV Pros and Cons\n",
    "➕ CSV is human-readable and easy to edit manually;\n",
    "\n",
    "➕ CSV provides a simple scheme;\n",
    "\n",
    "➕ CSV can be processed by almost all existing applications;\n",
    "\n",
    "➕ CSV is easy to implement and parse;\n",
    "\n",
    "➕ CSV is compact. For XML, you start a tag and end a tag for each column in each row. In CSV, the column headers are written only once;\n",
    "\n",
    "* No guarantees that data won't be missing or won't be in a different format.\n",
    "* Complex data structures need to be implemented using referncing into separate files\n",
    "\n",
    "➖ There is no standard way to present binary data;\n",
    "\n",
    "➖ Problems with CSV import (for example, no difference between NULL and quotes);\n",
    "\n",
    "➖ Poor support for special characters;\n",
    "\n",
    "➖ Lack of a universal standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb872ea",
   "metadata": {},
   "source": [
    "### JSON File Format\n",
    "\n",
    "* JSON (JavaScript Object Notation: open standard file format that uses human-readable text\n",
    "  * typically stored using .json extension. \n",
    "* Became popular as a space saving alterntive to XML\n",
    "* Inspired form JavaScript objected but is a language-independent data format. \n",
    "* very similar to python's lists and dicts\n",
    "* Also supported natively in Python\n",
    "  ```python\n",
    "  import json\n",
    "  # Do something with the json library\n",
    "  ```\n",
    "* The defacto language of the web\n",
    "  * Supported in all modern langues and particularly web languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11c99f",
   "metadata": {},
   "source": [
    "### JSON File Structure\n",
    "\n",
    "\n",
    "\n",
    "JSOn supports the following be of the following types.\n",
    "\n",
    "* Scalar values\n",
    "\n",
    "    * `Numbers`: e.g. 3 \n",
    "    \n",
    "    * `String`: Sequence of Unicode characters surrounded by double quotation marks.\n",
    "    \n",
    "    * `Boolean`: `true` or `false`.\n",
    "\n",
    "* Collections:\n",
    "\n",
    "    * `Array`: A list of values surrounded by square brackets, for example\n",
    "    * `Collections`: key\" value pairs separated by a comma(,)\n",
    "      *  Keys are String. value can be any valis other scalar or collection\n",
    "\n",
    "* See the following for more details: https://docs.fileformat.com/web/json/ \n",
    "* See the following very good (useful) validator for validating JSON files or records: https://jsonformatter.curiousconcept.com/#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16187bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'First Name': 'John',\n",
       "  'Occupation': 'Student',\n",
       "  'Salary': 120000,\n",
       "  'volunteer': False},\n",
       " {'First Name': 'John',\n",
       "  'Occupation': 'Student',\n",
       "  'salary': None,\n",
       "  'volunteer': True}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = [ \n",
    "    {'First Name': \"John\", \"Occupation\": \"Student\", \"Salary\": 120_000, \"volunteer\": False}, \n",
    "    {'First Name': \"John\", \"Occupation\": \"Student\", \"salary\": None, \"volunteer\": True}\n",
    "]\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cd4c7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"First Name\": \"John\", \"Occupation\": \"Student\", \"Salary\": 120000, \"volunteer\": false}, {\"First Name\": \"John\", \"Occupation\": \"Student\", \"salary\": null, \"volunteer\": true}]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "json.dumps(my_data)\n",
    "# Note the changes between the Python dict and the JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42c6ca",
   "metadata": {},
   "source": [
    "### Working with the Python `json` library\n",
    "\n",
    "\n",
    "* `All_Time_Worldwide_Box_Office_partial.json`  structure\n",
    "```json\n",
    "[\n",
    " {\n",
    "  \"Rank\": \"1\",\n",
    "  \"Year\": \"2009\",\n",
    "  \"Movie\": \"Avatar\",\n",
    "  \"WorldwideBox Office\": \"$2,845,899,541\",\n",
    "  \"DomesticBox Office\": \"$760,507,625\",\n",
    "  \"InternationalBox Office\": \"$2,085,391,916\"\n",
    " },\n",
    " {\n",
    "  \"Rank\": \"2\",\n",
    "  \"Year\": \"2019\",\n",
    "  \"Movie\": \"Avengers: Endgame\",\n",
    "  \"WorldwideBox Office\": \"$2,797,800,564\",\n",
    "  \"DomesticBox Office\": \"$858,373,000\",\n",
    "  \"InternationalBox Office\": \"$1,939,427,564\"\n",
    " },\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b546b462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Rank': '1',\n",
       "  'Year': '2009',\n",
       "  'Movie': 'Avatar',\n",
       "  'WorldwideBox Office': '$2,845,899,541',\n",
       "  'DomesticBox Office': '$760,507,625',\n",
       "  'InternationalBox Office': '$2,085,391,916'},\n",
       " {'Rank': '2',\n",
       "  'Year': '2019',\n",
       "  'Movie': 'Avengers: Endgame',\n",
       "  'WorldwideBox Office': '$2,797,800,564',\n",
       "  'DomesticBox Office': '$858,373,000',\n",
       "  'InternationalBox Office': '$1,939,427,564'},\n",
       " {'Rank': '3',\n",
       "  'Year': '1997',\n",
       "  'Movie': 'Titanic',\n",
       "  'WorldwideBox Office': '$2,207,986,545',\n",
       "  'DomesticBox Office': '$659,363,944',\n",
       "  'InternationalBox Office': '$1,548,622,601'},\n",
       " {'Rank': '4',\n",
       "  'Year': '2015',\n",
       "  'Movie': 'Star Wars Ep. VII: The Force Awakens',\n",
       "  'WorldwideBox Office': '$2,064,615,817',\n",
       "  'DomesticBox Office': '$936,662,225',\n",
       "  'InternationalBox Office': '$1,127,953,592'},\n",
       " {'Rank': '5',\n",
       "  'Year': '2018',\n",
       "  'Movie': 'Avengers: Infinity War',\n",
       "  'WorldwideBox Office': '$2,044,540,523',\n",
       "  'DomesticBox Office': '$678,815,482',\n",
       "  'InternationalBox Office': '$1,365,725,041'},\n",
       " {'Rank': '6',\n",
       "  'Year': '2015',\n",
       "  'Movie': 'Jurassic World',\n",
       "  'WorldwideBox Office': '$1,669,979,967',\n",
       "  'DomesticBox Office': '$652,306,625',\n",
       "  'InternationalBox Office': '$1,017,673,342'},\n",
       " {'Rank': '7',\n",
       "  'Year': '2019',\n",
       "  'Movie': 'The Lion King',\n",
       "  'WorldwideBox Office': '$1,654,367,425',\n",
       "  'DomesticBox Office': '$543,638,043',\n",
       "  'InternationalBox Office': '$1,110,729,382'},\n",
       " {'Rank': '8',\n",
       "  'Year': '2015',\n",
       "  'Movie': 'Furious 7',\n",
       "  'WorldwideBox Office': '$1,516,881,526',\n",
       "  'DomesticBox Office': '$353,007,020',\n",
       "  'InternationalBox Office': '$1,163,874,506'},\n",
       " {'Rank': '9',\n",
       "  'Year': '2012',\n",
       "  'Movie': 'The Avengers',\n",
       "  'WorldwideBox Office': '$1,515,100,211',\n",
       "  'DomesticBox Office': '$623,357,910',\n",
       "  'InternationalBox Office': '$891,742,301'},\n",
       " {'Rank': '10',\n",
       "  'Year': '2019',\n",
       "  'Movie': 'Frozen II',\n",
       "  'WorldwideBox Office': '$1,446,925,396',\n",
       "  'DomesticBox Office': '$477,373,578',\n",
       "  'InternationalBox Office': '$969,551,818'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_file = open('../Data/All_Time_Worldwide_Box_Office_partial.json') \n",
    "\n",
    "movies_data = json.load(json_file)\n",
    "movies_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7341ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d5c94f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eac5ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie Avatar, grossed $2,845,899,541 in 2009\n",
      "The movie Avengers: Endgame, grossed $2,797,800,564 in 2019\n",
      "The movie Titanic, grossed $2,207,986,545 in 1997\n",
      "The movie Star Wars Ep. VII: The Force Awakens, grossed $2,064,615,817 in 2015\n",
      "The movie Avengers: Infinity War, grossed $2,044,540,523 in 2018\n",
      "The movie Jurassic World, grossed $1,669,979,967 in 2015\n",
      "The movie The Lion King, grossed $1,654,367,425 in 2019\n",
      "The movie Furious 7, grossed $1,516,881,526 in 2015\n",
      "The movie The Avengers, grossed $1,515,100,211 in 2012\n",
      "The movie Frozen II, grossed $1,446,925,396 in 2019\n"
     ]
    }
   ],
   "source": [
    "for record in movies:\n",
    "    print(f\"The movie {record['Movie']}, grossed {record['WorldwideBox Office']} in {record['Year']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2091ef8",
   "metadata": {},
   "source": [
    "### JSON Pros and Cons\n",
    "\n",
    "* Pros: \n",
    "    * Very well supported in modern languages and technologies, infrastructures \n",
    "    * Can be used as the basis for more performance-optimized formats Parquet or Avro (discussed next)\n",
    "    * Supports hierarchical structures abstracting the need for complex relationships\n",
    "    * The *defacto* standard in NoSQL databases\n",
    "* Cons:\n",
    "    * Much small footprint than XML but still fairly large due to repeated field names\n",
    "    * Difficult to split without loading into memory first\n",
    "    * Not easy to index\n",
    "    * Some tentatives to add a schema but not commonly used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d7204",
   "metadata": {},
   "source": [
    "### AVRO File Format\n",
    "\n",
    "* AVRO format is an advanced form of JSON format\n",
    "    * Leverages some of the advantages of JSON while mitigating some of its disadvantages\n",
    "* Uses a JSON definition (schma) and description in addition to the data without the repeated field names.\n",
    "  * Said to be self-describng because you can include the schema and documentation in the header of the file containing the data\n",
    "  * Is row-oriented; each entry is an instance of the data\n",
    "\n",
    "* Released by the Hadoop working group in 2009 to use with Hadoop Systems\n",
    "* It is a row-based format that has a high degree of splitting\n",
    "* Provides mechanism to manage scheme evolution \n",
    "* Supports schema evolution\n",
    "\n",
    "* Python need a library that understand the binary format used.\n",
    "  & availble or most modern languages, including Python\n",
    "  & We will use `avro` library in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1779595",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "Pros:\n",
    "    * binary data minimizes file size and maximizes efficiency.\n",
    "    * Avro has reliable support for schema evolution by managing added, missing, and changed fields.\n",
    "    * This allows old software to read new data, and new software to read old data — it is a critical feature if your data can change.\n",
    "    * Supports schema evolution\n",
    "\n",
    "* Cons: \n",
    "    * Data is not human radable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1ec12b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting avro\n",
      "  Downloading avro-1.10.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 811 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: avro\n",
      "  Building wheel for avro (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro: filename=avro-1.10.2-py3-none-any.whl size=96830 sha256=f956e98a2543b7c14c1ad70713c410a1d0e9aafb0bfdd61c27743181f24b73e5\n",
      "  Stored in directory: /Users/mahdi/Library/Caches/pip/wheels/66/b5/b3/185a0da0ecbc3e902e24d1e2fa415db0c7342d6e3633c49d30\n",
      "Successfully built avro\n",
      "Installing collected packages: avro\n",
      "Successfully installed avro-1.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0ce0c",
   "metadata": {},
   "source": [
    "### PARQUET Format\n",
    "\n",
    "\n",
    "* Parquet was developed by Twitter and Cloudera as columnar data store\n",
    "* Parquet is especially useful with wide datasets (datasets with manu columns)\n",
    "\n",
    "* optimized for reading and is therefore ideal read-intensive workloads\n",
    "* Parquet was also designed to support columnar partition\n",
    "    * Splitting the data based on value similarity\n",
    "    * E.g.: split on the similar values of the MONTH in a transactions dataset\n",
    "    * Splits can be nested, but splitting on a second attribute. Will result in a nested folder hierarchy\n",
    "        making reading a subset odf keys very easy\n",
    "      \n",
    "```      \n",
    "    MONTH=JANUARY\n",
    "        CITY=HONOLULU\n",
    "           data..\n",
    "        CITY=MONTREAL\n",
    "            data..\n",
    "        CITY=NY\n",
    "            data..\n",
    "        \n",
    "    MONTH=FEBRUARY\n",
    "        CITY=HONOLULU\n",
    "           data..\n",
    "        CITY=MONTREAL\n",
    "            data..\n",
    "        CITY=NY\n",
    "            data..\n",
    "\n",
    "    ...\n",
    "      \n",
    "```  \n",
    "\n",
    "https://blog.datasyndrome.com/python-and-parquet-performance-e71da65269ce\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARQUET PROS and CONS\n",
    "\n",
    "Pros: \n",
    " * Highly compressable and since data is stored columnn-wise (compression rates up to 75%)\n",
    "   * can use different compression algorithm with different datatypes\n",
    " * Seamless splittability across columns.\n",
    " * Optimized for reading data and idea for read-intensive tasks\n",
    "   * Can use parallelization to read different column.\n",
    " * Data is self-describing, i.e., schema is included in with the data\n",
    "Cons:\n",
    " * Very slow at writing data and not good with write-intensive applications\n",
    " * Does not suppport updates on the data as Parquet files are immutable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
