{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449f7aab",
   "metadata": {},
   "source": [
    "### Popular Big Data Fromats\n",
    "\n",
    "* Data format is an important aspect of working with big data\n",
    "\n",
    "* The rolling topic is \"There ain't such a thing as free lunch\"\n",
    "\n",
    "```\"There ain't no such thing as a free lunch\" (TANSTAAFL), also known as \"there is no such thing as a free lunch\" (TINSTAAFL), is an expression that describes the cost of decision-making and consumption. The expression conveys the idea that things appearing free always have some cost paid by somebody, or that nothing in life is truly free. ``` **https://www.investopedia.com/terms/t/tanstaafl.asp**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce05a6",
   "metadata": {},
   "source": [
    "### Popular Big Data Fromats\n",
    "\n",
    "* Some of the issues that arise when working with data formats is:\n",
    "\n",
    "1. Compression\n",
    "  * Not all file formats are equally compressible with the same algorithm.\n",
    "2. Splittability\n",
    "  * How splittable is a file format.\n",
    "  * Being able to split a file and run across multiple machines can be critical in some instances\n",
    "3. Columnar and row-wise data formats\n",
    "  * Being able to compute column-based stats can force the adoption of a format that make it easier to extract column data\n",
    "  * How different data formats affect the wrangling of big data\n",
    "4. Data Types and Schema Evolution.\n",
    "  * Do we need to enforce data types?\n",
    "  * Will my data format change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed420e46",
   "metadata": {},
   "source": [
    "### File Format: a Quick intuition\n",
    "\n",
    "* In big data, the right storage format is paramount for achieving performance, saving space and making certain operations possible.\n",
    "\n",
    "* Can save time, cost, improve computation time etc.\n",
    "\n",
    "* We're accustomed to row-based formats\n",
    "\n",
    "  * MS Excel file-like where each row is a table entry\n",
    "\n",
    "| Transaction Date     | Nb Items     | Total       |\n",
    "|------------------    |----------    |---------    |\n",
    "| 01/01/2001           | 4            | 1852.14     |\n",
    "| 01/01/2001           | 3            | 968.00      |\n",
    "| `...`             | `...`     | `...`     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8017d4a",
   "metadata": {},
   "source": [
    "### File Format: a Quick intuition - Cont'd\n",
    "    \n",
    "* This format may be inappropriate for certain types of data or operations\n",
    "\n",
    "* Imagine that the sales info above contains hundreds of millions of transactions with hundreds of thousands of transactions each day\n",
    "    * The same transaction dates will be unnecessarily duplicated hundrend of thousands of time.\n",
    "    * Perhaps a dictionary like format where the key is the date would help save on storage\n",
    " \n",
    "```python\n",
    "# notice the two ellipses.\n",
    "{\"01/01/2001\": ((4, 1852.14), (3,  968.00), ...), ...}\n",
    "```\n",
    "  * This will also be more efficient to compute operations on days\n",
    "   * E.g. count number of transactions or total sales per day\n",
    "   * What about computing the running sales total?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4e83b",
   "metadata": {},
   "source": [
    "### File Format: a Quick intuition - Cont'd\n",
    "\n",
    "* Imagine that objectives if to compute the total sales\n",
    "  * We need to read millions of lines to compute a single values\n",
    "* Perhaps we can store the data as row data. Reading a single line is sufficient to compute the average.\n",
    "\n",
    "|              |      |  |   |\n",
    "| :---              |    :----:  | :--------: |:---:|\n",
    "| **Totals**             | 1852.14    | 968.00     | `...` |\n",
    "| **Transaction Dates** | 01/01/2001 | 01/01/2001 | `...` |\n",
    "| **Nb Items**               | 4             | 3          | `...` |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffbb871e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3a10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dd921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals are: [8410.51478950048, 270.75336741050046, 4681.8751930660665, 2213.7419153776336, 364.7501964370208, 2971.793305945331, 9781.556556740885, 273.1911617646127, 5446.13524865059, 3727.1516001030936]\n",
      "The transaction : ['2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08', '2021-09-08']\n",
      "The number items: [2, 23, 24, 6, 21, 40, 9, 24, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import date\n",
    "\n",
    "totals = []\n",
    "transaction_dates = []\n",
    "nb_items = []\n",
    "\n",
    "for i in range(10):\n",
    "    totals.append(random.uniform(10, 10000))\n",
    "    transaction_dates.append(str(date.today()))\n",
    "    nb_items.append(random.randint(0,40))\n",
    "\n",
    "print(f\"Totals are: {totals}\")\n",
    "print(f\"The transaction : {transaction_dates}\")\n",
    "print(f\"The number items: {nb_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9047e7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38141.46333499621"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2fe920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021-09-08', 2, 8410.51478950048),\n",
       " ('2021-09-08', 23, 270.75336741050046),\n",
       " ('2021-09-08', 24, 4681.8751930660665),\n",
       " ('2021-09-08', 6, 2213.7419153776336),\n",
       " ('2021-09-08', 21, 364.7501964370208),\n",
       " ('2021-09-08', 40, 2971.793305945331),\n",
       " ('2021-09-08', 9, 9781.556556740885),\n",
       " ('2021-09-08', 24, 273.1911617646127),\n",
       " ('2021-09-08', 4, 5446.13524865059),\n",
       " ('2021-09-08', 0, 3727.1516001030936)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(transaction_dates, nb_items, totals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14cc7fe",
   "metadata": {},
   "source": [
    "### Decisions for File Formats\n",
    "\n",
    "* There 4 consideration when selecting file fomats:\n",
    "    1. Row vs Column\n",
    "    2. Schema Management\n",
    "    3. Spilitability\n",
    "    4. Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2bb0e",
   "metadata": {},
   "source": [
    "### 1. Row- and Column-Based Formats\n",
    "\n",
    "\n",
    "* An important consideration when selecting a big data format\n",
    "\n",
    "* Row-based: Ideal when using all the data\n",
    "  * example, building a machine learning model that requires all the features and all the isntances\n",
    "    * Avoid reading all the dataset in RAM by loading chunks at a time\n",
    "\n",
    "* Column-based storage: useful when performing operations on a subset of columns\n",
    "  * Computing total sales, or computing a total aggregated by aggregated by date, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b5853",
   "metadata": {},
   "source": [
    "### Row-Based Formats\n",
    "\n",
    "* Simplest form of data\n",
    "* Used in most mainstream applications, from web log files to highly-structured database systems like MySql and Oracle.\n",
    "\n",
    "* Processing all the data would require reading all inputs line by line\n",
    "\n",
    "* This is commonly used for Online Transactional Processing (OLTP).\n",
    "  * OLTP systems usually process CRUD queries (Create, Read, Update and Delete) at a record level.\n",
    "  * The main emphasis for OLTP systems is maintaining data integrity in multi-access environments\n",
    "* Effectiveness measured by the number of transactions per second\n",
    "   * More on this when we discuss big data platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e001a",
   "metadata": {},
   "source": [
    "### Column Based Formats\n",
    "\n",
    "\n",
    "* Data is grouped by columns\n",
    "* Easy to focus computation on specific columns of data\n",
    "  * Ex. Search for largest values is easier since data is stored sequentially by column.\n",
    " \n",
    "* Ideal for compression\n",
    "  * Compression codecs (e.g., GZIP, pkzip, etc..) have a higher compression-ratio when compressing sequences of similar data. \n",
    "  \n",
    "```[1,2,...], [\"John\", \"Janet\"], [\"Doe\", \"Smith\",...], [\"125,000\", \"195,129\", ...]```\n",
    "\n",
    "* If much more efficient than compressing:\n",
    "\n",
    "```[[1, \"John\", \"Doe\", \"125,000\"], [2, \"Janet\", \"Smith\", \"195,129\"], ...]```\n",
    "\n",
    "* This way of processing data is usually called OLAP (OnLine Analytical Processing)   \n",
    " * OLAP is an approach designed to quickly answer analytics queries involving multiple dimensions (features)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf15a2b",
   "metadata": {},
   "source": [
    "### Compression of Row vs. Columnar Data\n",
    "\n",
    "  * Let's do an experiment\n",
    "    * We'll perform such small experiments a lot to get a feel for Python, theory and the concepts covered.\n",
    "    \n",
    "  * Typically, the slowest components in large distributed systems are the disk and network\n",
    "      * Using compression reduces read IO and transfers, thus speeding up the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b956e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 4, 1, 1, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices([1,2,3,4], k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a583d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'C', 'T', 'T', 'A', 'G']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choices(\"ACGT\", k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e5d052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "0123456789\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "print(string.printable)\n",
    "print(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0b23c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib \n",
    "import string\n",
    "\n",
    "# let's randomly generate two string of 1000, an ASCII and an INT\n",
    "\n",
    "random_ASCII = random.choices(string.printable, k=10_000)\n",
    "random_numbers = random.choices(string.digits, k=10_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc0f4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8412\n",
      "5085\n"
     ]
    }
   ],
   "source": [
    "print(len(zlib.compress( str.encode(\"\".join(random_ASCII)))))\n",
    "print(len(zlib.compress( str.encode(\"\".join(random_numbers)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279abc9",
   "metadata": {},
   "source": [
    "### Column-based formats: Advantages and Disadvantages\n",
    "\n",
    "\n",
    "<u>Advantages</u>:\n",
    "* Columnar-storage of data can yield sometimes 100x- 1000x performance improvement, specifically on wide data\n",
    "    * Wide data = data with a very large number of columns (or more columns than observations)\n",
    "\n",
    "<u>Disadvantages</u>:\n",
    "  * Hard to read by a human\n",
    "  * Can be more CPU intensive to write for very large data.\n",
    "    * Need to collect the data for each column before writing it to file\n",
    "  * Difficult to access a single instnace (entry across all values)\n",
    "   * Need to parse all columns to position $i$\n",
    "  * Not efficient with CRUD operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a4a3e",
   "metadata": {},
   "source": [
    "### 2- Datatype and Schema Enforcement and Evolution\n",
    "\n",
    "* \"Schema\" in a database context, means the structure and organization of the data  \n",
    "    * Structure: datatypes, missing values, primary keys, etc, indices, etc.\n",
    "    * Organization: relationships across tables.\n",
    "\n",
    "* Here, we mostly refer to the data type\n",
    "\n",
    "* In text format, (e.g.: table with values separated by space), datatype cannot be declared or enforced\n",
    "\n",
    "* Declaring the type of a value provides some advantages.\n",
    "  * Storage requirements: String will require more storage than boolean (2 bytes)\n",
    "  * Data validity: guarantees that the dataset is valid\n",
    "  * Compression: We know how to compress different types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c5229",
   "metadata": {},
   "source": [
    "### 2- Datatype & Schema Enforcement and Evolution - Cont'd\n",
    "\n",
    "\n",
    "* In the event that there is no guarantee that data won't change in the future, you may need to consider schema evolution.\n",
    "\n",
    "\n",
    "* When evaluating schema evolution, there are a few key questions to ask of any data format:\n",
    "  * How easy is it to update a schema (such as adding a field, removing or renaming a field)?\n",
    "  * How will different versions of the schema “talk” to each other?\n",
    "  * How fast can the schema be processed?\n",
    "  * How does it impact the size of data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de7933",
   "metadata": {},
   "source": [
    "### 3- Splitability\n",
    "\n",
    "* Big data can often comprise many millions of records.\n",
    "  * Think of instance monthly logs, yearly transactions, daily airplane sensors recording\n",
    "\n",
    "* Often useful to split the data across multiple machines and execute each computation separately\n",
    "\n",
    "* Some file formats are more amenable to splitting than others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebb546",
   "metadata": {},
   "source": [
    "### 3- Splitability - Row-based\n",
    "\n",
    "Row-based formats can be split along row boundaries\n",
    "\n",
    "```\n",
    "# file 1  with n lines\n",
    "01/01/2001           4            1852.14\n",
    "01/01/2001           3            968.00\n",
    "...\n",
    "```\n",
    "\n",
    "* Splitting can be done\n",
    "  * Randomly plitting `file 1` with `n` observations across `m` total machines is easy.\n",
    "\n",
    "    * Each machine gets `ceiling(n/m)` unique lines, last machine gets remaining lines\n",
    "\n",
    " * To group items across one or more fields:\n",
    " \n",
    "    * Partitioning over particular column values can be difficult if data is stored in a random order.\n",
    "    * May require sorting the data first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f3d5a",
   "metadata": {},
   "source": [
    "### 3- Splitability row-based, nested \n",
    "\n",
    "* Some files formats are more amenable to splitting than others.\n",
    "\n",
    "\n",
    "``` \n",
    "file 2\n",
    "{\"01/01/20014\": [(4, 1852.14), (3, 968.00)], ....}\n",
    "```\n",
    "\n",
    "* You cannot easily split this file this file format without parsing the file first.\n",
    "  * Need to read the compelte file to split it into chunks.\n",
    "    * Data may need to ne loaded in RAM first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aad4d4",
   "metadata": {},
   "source": [
    "### 3- Splitability: Column-based, nested\n",
    "\n",
    "\n",
    "* A column-based format can be split if the comutation is column-specific.\n",
    "\n",
    "```\n",
    "# file 3\n",
    "date: 01/01/2001, 01/01/2001\n",
    "nb_items: 4, 3\n",
    "totals: 1852.14, 968.00\n",
    "```\n",
    "\n",
    "Splitting can only e done column-wise:\n",
    "* In the example above, each machine is concerned with a computation on a specific variable. For example:\n",
    "  * Machine 1 takes `date` data and computes the number of sales per month\n",
    "  * Machine 2 takes the `nb_items` data and computes the total number of sales\n",
    "  * Machine 3 takes the `totals` data and computes the total sales values\n",
    " \n",
    "* Machines don't have any knowledge of variables that are not given.\n",
    "  * E.g. machine three is not given date info and cannot compute, for example, the monthly or weekly sales average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3a8c1",
   "metadata": {},
   "source": [
    "### 4- Compression\n",
    "\n",
    "\n",
    "* When working on a distributed system, data transfer can be a serious bottleneck\n",
    "* Compression can substantially improve runtime and storage requirements\n",
    "\n",
    "* We illustrated \"naively\" that columnar data can achieve better compression rates than row-based data\n",
    "  * Simple way to think about it: column will have a lot more duplicate values:\n",
    "      * Ex. Age Column: 21, 22, 21, 24, 25, 21, 22, 21, 19, 21, 21, 22, ....\n",
    "      \n",
    "* Note that complex compression algorithms on very large files can save on space but substantially increase compute time.\n",
    "    * Uncompression/re-compression needs to occur every time you need to access the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6279c",
   "metadata": {},
   "source": [
    "### Standardization and File Formats\n",
    "\n",
    "* Naturally, one can choose thier own format file format\n",
    "  * Many companies may choose to do so internally for many reasons.\n",
    "  * E.g.:\n",
    "\n",
    "```\n",
    "FIRST_NAME_1\\sLAST_NAME_1\\tFIRST_NAME_2\\sLAST_NAME_2\\tFIRST_NAME_3\\sLAST_NAME_3...\n",
    "JOBTITLE_1\\sSALARY_1\\tJOBTITLE_2\\sSALARY_2\\tJOBTITLE_3\\sSALARY_3\n",
    "```\n",
    "\n",
    "* However, there are many benefits to using a standard file format. E.g.:\n",
    "  * Clarity: eliminating the need for guesswork or extra searching\n",
    "  * Quality: stndard formats are designed by large teams and used extensively, which provides opportunities to find and correct bugs\n",
    "  * Productivity: no need to maintain internal documentation, which is easier to get answer online when issues arise.\n",
    "  * Interoperability: you data is no longer locked to your company. Can be used acros platforms.\n",
    "\n",
    "* Some of the most used formats are CSV, JSON, Parquet, AVRO HDF5\n",
    "  * Very well supported in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0ff77",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* Files in the CSV (Comma-serparated values) format are usually used to exchange tabular dat\n",
    "  * Plain-text file (readable characters)\n",
    " \n",
    "* CSV is a row-based file format: each row of the file is a separate data instance\n",
    "  * May or may not contain a header\n",
    "* Structure is conveyed through explicit commas\n",
    "  * Text commas are encapsulated in double quotes\n",
    "\n",
    "```\n",
    "Title,Author,Genre,Height,Publisher\n",
    "\"Computer Vision, A Modern Approach\",\"Forsyth, David\",data_science,255,Pearson\n",
    "Data Mining Handbook,\"Nisbet, Robert\",data_science,242,Apress\n",
    "Making Software,\"Oram, Andy\",computer_science,232,O'Reilly\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae2028",
   "metadata": {},
   "source": [
    "### CSV File Format\n",
    "\n",
    "* CSV format is not fully standardized\n",
    "  * files may be sepated by other chatacters such as tabs (tsv) or spaces (ssv)\n",
    " \n",
    "* Data connections are usually established using multiple CSV files.\n",
    "   * Uses foreign keys (specific columns) across files\n",
    "   * Connection not expressed in the file format\n",
    " \n",
    "* Data Strucure conveyed through redundant values across files\n",
    "\n",
    "* Native support in Python\n",
    "```python\n",
    "import csv\n",
    "csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "# use csv ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e31127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: ['Rank', 'Year', 'Movie', 'WorldwideBox Office', 'DomesticBox Office', 'InternationalBox Office']\n",
      "Line 1: ['1', '2009', 'Avatar', '$2,845,899,541', '$760,507,625', '$2,085,391,916']\n",
      "Line 2: ['2', '2019', 'Avengers: Endgame', '$2,797,800,564', '$858,373,000', '$1,939,427,564']\n",
      "Line 3: ['3', '1997', 'Titanic', '$2,207,986,545', '$659,363,944', '$1,548,622,601']\n",
      "Line 4: ['4', '2015', 'Star Wars Ep. VII: The Force Awakens', '$2,064,615,817', '$936,662,225', '$1,127,953,592']\n",
      "Line 5: ['5', '2018', 'Avengers: Infinity War', '$2,044,540,523', '$678,815,482', '$1,365,725,041']\n",
      "Line 6: ['6', '2015', 'Jurassic World', '$1,669,979,967', '$652,306,625', '$1,017,673,342']\n",
      "Line 7: ['7', '2019', 'The Lion King', '$1,654,367,425', '$543,638,043', '$1,110,729,382']\n",
      "Line 8: ['8', '2015', 'Furious 7', '$1,516,881,526', '$353,007,020', '$1,163,874,506']\n",
      "Line 9: ['9', '2012', 'The Avengers', '$1,515,100,211', '$623,357,910', '$891,742,301']\n"
     ]
    }
   ],
   "source": [
    "# All_Time_Worldwide_Box_Office_partial.csv\n",
    "import csv\n",
    "csvfile = open('data/All_Time_Worldwide_Box_Office.csv') \n",
    "movies_file = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "i = 0 \n",
    "for line in movies_file:\n",
    "    print(f\"Line {i}: {line}\")\n",
    "    i+=1\n",
    "    if i ==10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70a5b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: {'Rank': '1', 'Year': '2009', 'Movie': 'Avatar', 'WorldwideBox Office': '$2,845,899,541', 'DomesticBox Office': '$760,507,625', 'InternationalBox Office': '$2,085,391,916'}\n",
      "Line 1: {'Rank': '2', 'Year': '2019', 'Movie': 'Avengers: Endgame', 'WorldwideBox Office': '$2,797,800,564', 'DomesticBox Office': '$858,373,000', 'InternationalBox Office': '$1,939,427,564'}\n",
      "Line 2: {'Rank': '3', 'Year': '1997', 'Movie': 'Titanic', 'WorldwideBox Office': '$2,207,986,545', 'DomesticBox Office': '$659,363,944', 'InternationalBox Office': '$1,548,622,601'}\n",
      "Line 3: {'Rank': '4', 'Year': '2015', 'Movie': 'Star Wars Ep. VII: The Force Awakens', 'WorldwideBox Office': '$2,064,615,817', 'DomesticBox Office': '$936,662,225', 'InternationalBox Office': '$1,127,953,592'}\n",
      "Line 4: {'Rank': '5', 'Year': '2018', 'Movie': 'Avengers: Infinity War', 'WorldwideBox Office': '$2,044,540,523', 'DomesticBox Office': '$678,815,482', 'InternationalBox Office': '$1,365,725,041'}\n",
      "Line 5: {'Rank': '6', 'Year': '2015', 'Movie': 'Jurassic World', 'WorldwideBox Office': '$1,669,979,967', 'DomesticBox Office': '$652,306,625', 'InternationalBox Office': '$1,017,673,342'}\n",
      "Line 6: {'Rank': '7', 'Year': '2019', 'Movie': 'The Lion King', 'WorldwideBox Office': '$1,654,367,425', 'DomesticBox Office': '$543,638,043', 'InternationalBox Office': '$1,110,729,382'}\n",
      "Line 7: {'Rank': '8', 'Year': '2015', 'Movie': 'Furious 7', 'WorldwideBox Office': '$1,516,881,526', 'DomesticBox Office': '$353,007,020', 'InternationalBox Office': '$1,163,874,506'}\n",
      "Line 8: {'Rank': '9', 'Year': '2012', 'Movie': 'The Avengers', 'WorldwideBox Office': '$1,515,100,211', 'DomesticBox Office': '$623,357,910', 'InternationalBox Office': '$891,742,301'}\n",
      "Line 9: {'Rank': '10', 'Year': '2019', 'Movie': 'Frozen II', 'WorldwideBox Office': '$1,446,925,396', 'DomesticBox Office': '$477,373,578', 'InternationalBox Office': '$969,551,818'}\n"
     ]
    }
   ],
   "source": [
    "# All_Time_Worldwide_Box_Office_partial.csv\n",
    "import csv\n",
    "csvfile = open('data/All_Time_Worldwide_Box_Office.csv') \n",
    "movies_file = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "i = 0 \n",
    "for line in movies_file:\n",
    "    print(f\"Line {i}: {line}\")\n",
    "    i+=1\n",
    "    if i ==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a748ae3",
   "metadata": {},
   "source": [
    "### CSV Pros and Cons\n",
    "<u>Pros:</u>\n",
    "* CSV is human-readable and easy to edit manually\n",
    "* CSV provides a simple scheme\n",
    "* CSV can be processed by almost all existing applications\n",
    "* CSV is easy to implement and parse;\n",
    "* CSV is compact (compared to, for instance JSON or MXL)\n",
    "* Column headers are written only once\n",
    "\n",
    "<u>Cons:</u>\n",
    "* No guarantees that data won't be missing or won't be in a different format.\n",
    "* No way to implement complex data structures\n",
    "  * May need to rerences other files to implement nesting\n",
    "* There is no standard way to present binary data;\n",
    "* Poor support for special characters;\n",
    "* Lack of a universal standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb872ea",
   "metadata": {},
   "source": [
    "### JSON File Format\n",
    "\n",
    "* JSON (JavaScript Object Notation)\n",
    "\n",
    "* open standard file format that uses human-readable text\n",
    "  * FIle typically stored using `.json` extension.\n",
    "\n",
    "* Became popular as a space-saving alternative to XML\n",
    "\n",
    "* Inspired by JavaScript objects but is a language-independent data format.\n",
    "\n",
    "* Very similar to the combination of Python's lists and dicts\n",
    "\n",
    "* Also supported natively in Python\n",
    "  ```python\n",
    "  import json\n",
    "json.load(...)\n",
    "  ```\n",
    "* The defacto language of the web\n",
    "  * Supported in all modern languages and particularly web languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11c99f",
   "metadata": {},
   "source": [
    "### JSON File Structure\n",
    "\n",
    "* JSOn supports the following be of the following types.\n",
    "\n",
    "* Scalar values\n",
    "\n",
    "    * `Numbers`: e.g. 3\n",
    "    \n",
    "    * `String`: Sequence of Unicode characters surrounded by double quotation marks.\n",
    "    \n",
    "    * `Boolean`: `true` or `false`.\n",
    "\n",
    "* Collections:\n",
    "\n",
    "    * `Array`: A list of values surrounded by square brackets `[]`\n",
    "    * `Dictionaries`: key\" value pairs separated by a comma(,) and enclosed in `{}`\n",
    "      *  Keys are String. value can be any valid scalar or collection\n",
    "\n",
    "* See the following for more details: https://docs.fileformat.com/web/json/\n",
    "* See the following very good (useful) validator for validating JSON files or records: https://jsonformatter.curiousconcept.com/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16187bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'First Name': 'John',\n",
       "  'Occupation': 'Student',\n",
       "  'Salary': 120000,\n",
       "  'volunteer': False},\n",
       " {'First Name': 'John',\n",
       "  'Occupation': 'Student',\n",
       "  'salary': None,\n",
       "  'volunteer': True}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = [ \n",
    "    {'First Name': \"John\", \"Occupation\": \"Student\", \"Salary\": 120_000, \"volunteer\": False}, \n",
    "    {'First Name': \"John\", \"Occupation\": \"Student\", \"salary\": None, \"volunteer\": True}\n",
    "]\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd97821",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsom.dump(s)\n",
    "json.dumps()\n",
    "json.load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cd4c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"First Name\": \"John\", \"Occupation\": \"Student\", \"Salary\": 120000, \"volunteer\": false}, {\"First Name\": \"John\", \"Occupation\": \"Student\", \"salary\": null, \"volunteer\": true}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "json_representation = json.dumps(my_data)\n",
    "print(json_representation)\n",
    "# Note the changes between the Python dict and the JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42c6ca",
   "metadata": {},
   "source": [
    "### Working with the Python `json` library\n",
    "\n",
    "\n",
    "* `All_Time_Worldwide_Box_Office_partial.json`  structure\n",
    "```json\n",
    "[\n",
    " {\n",
    "  \"Rank\": \"1\",\n",
    "  \"Year\": \"2009\",\n",
    "  \"Movie\": \"Avatar\",\n",
    "  \"WorldwideBox Office\": \"$2,845,899,541\",\n",
    "  \"DomesticBox Office\": \"$760,507,625\",\n",
    "  \"InternationalBox Office\": \"$2,085,391,916\"\n",
    " },\n",
    " {\n",
    "  \"Rank\": \"2\",\n",
    "  \"Year\": \"2019\",\n",
    "  \"Movie\": \"Avengers: Endgame\",\n",
    "  \"WorldwideBox Office\": \"$2,797,800,564\",\n",
    "  \"DomesticBox Office\": \"$858,373,000\",\n",
    "  \"InternationalBox Office\": \"$1,939,427,564\"\n",
    " },\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b546b462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Rank': '1',\n",
       "  'Year': '2009',\n",
       "  'Movie': 'Avatar',\n",
       "  'WorldwideBox Office': '$2,845,899,541',\n",
       "  'DomesticBox Office': '$760,507,625',\n",
       "  'InternationalBox Office': '$2,085,391,916'},\n",
       " {'Rank': '2',\n",
       "  'Year': '2019',\n",
       "  'Movie': 'Avengers: Endgame',\n",
       "  'WorldwideBox Office': '$2,797,800,564',\n",
       "  'DomesticBox Office': '$858,373,000',\n",
       "  'InternationalBox Office': '$1,939,427,564'},\n",
       " {'Rank': '3',\n",
       "  'Year': '1997',\n",
       "  'Movie': 'Titanic',\n",
       "  'WorldwideBox Office': '$2,207,986,545',\n",
       "  'DomesticBox Office': '$659,363,944',\n",
       "  'InternationalBox Office': '$1,548,622,601'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_file = open('data/All_Time_Worldwide_Box_Office_partial.json') \n",
    "\n",
    "movies_data = json.load(json_file)\n",
    "movies_data[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7341ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d5c94f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movies_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eac5ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie Avatar, grossed $2,845,899,541 in 2009\n",
      "The movie Avengers: Endgame, grossed $2,797,800,564 in 2019\n",
      "The movie Titanic, grossed $2,207,986,545 in 1997\n",
      "The movie Star Wars Ep. VII: The Force Awakens, grossed $2,064,615,817 in 2015\n",
      "The movie Avengers: Infinity War, grossed $2,044,540,523 in 2018\n",
      "The movie Jurassic World, grossed $1,669,979,967 in 2015\n",
      "The movie The Lion King, grossed $1,654,367,425 in 2019\n",
      "The movie Furious 7, grossed $1,516,881,526 in 2015\n",
      "The movie The Avengers, grossed $1,515,100,211 in 2012\n",
      "The movie Frozen II, grossed $1,446,925,396 in 2019\n"
     ]
    }
   ],
   "source": [
    "for record in movies:\n",
    "    print(f\"The movie {record['Movie']}, grossed {record['WorldwideBox Office']} in {record['Year']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2091ef8",
   "metadata": {},
   "source": [
    "### JSON Pros and Cons\n",
    "\n",
    "* Pros:\n",
    "    * Very well supported in modern languages, technologies and infrastructures\n",
    "    * Can be used as the basis for more performance-optimized formats Parquet or Avro (discussed next)\n",
    "    * Supports hierarchical structures abstracting the need for complex relationships\n",
    "    * The *defacto* standard in NoSQL databases\n",
    "* Cons:\n",
    "    * Much smaller footprint than XML but still fairly large due to repeated field names\n",
    "    * Difficult to split without loading into memory first\n",
    "    * Not easy to index\n",
    "    * Some tentatives to add a schema but not commonly used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d7204",
   "metadata": {},
   "source": [
    "### AVRO File Format\n",
    "\n",
    "* AVRO format is an advanced form of JSON format\n",
    "    * Leverages some of the advantages of JSON while mitigating some of its disadvantages\n",
    "* Uses a JSON definition (schema) and description in addition to the data without the repeated field names.\n",
    "  * Said to be self-descriptive because you can include the schema and documentation in the header of the file containing the data\n",
    "  * Is row-oriented; each entry is an instance of the data\n",
    "\n",
    "* Released by the Hadoop working group in 2009 to use with Hadoop Systems\n",
    "* It is a row-based format that has a high degree of splitting\n",
    "* Provides mechanism to manage schema evolution\n",
    "* Supports schema evolution\n",
    "\n",
    "* Python needs a library that understands the binary format used.\n",
    "  * support for most modern languages, including Python\n",
    "  * We will use `avro` library in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1779595",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "Pros:\n",
    "    * Binary data minimizes file size and maximizes efficiency\n",
    "    * Avro has reliable support for schema evolution\n",
    "      * Supports new missing, or changed fields.\n",
    "      * This allows old software to read new data, and new software to read old data\n",
    "      * It is a critical feature if your data can change.\n",
    "* Cons:\n",
    "    * Data is not human readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ec12b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: avro in /opt/anaconda3/lib/python3.8/site-packages (1.10.2)\r\n"
     ]
    }
   ],
   "source": [
    "# You can install form Jupyter Notebook\n",
    "!pip install avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0ce0c",
   "metadata": {},
   "source": [
    "### PARQUET Format\n",
    "\n",
    "\n",
    "* Parquet was developed by Twitter and Cloudera as a columnar data store\n",
    "* Parquet is especially useful with wide datasets (datasets with many columns)\n",
    "\n",
    "* Optimized for reading and is therefore ideal for read-intensive workloads\n",
    "* Parquet was also designed to support columnar partitions\n",
    "    * Splitting the data based on value similarity,w hcih results in a folder hierarchy\n",
    "      * E.g.: split on the similar values of the MONTH  or department\n",
    "    * Splits can be nested by splitting on a second attribute.\n",
    "      * Will result in a nested folder hierarchy\n",
    "     \n",
    "```      \n",
    "    MONTH=JANUARY\n",
    "        CITY=HONOLULU\n",
    "           data..\n",
    "        CITY=MONTREAL\n",
    "            data..\n",
    "        CITY=NY\n",
    "            data..\n",
    "        \n",
    "    MONTH=FEBRUARY\n",
    "        CITY=HONOLULU\n",
    "           data..\n",
    "        CITY=MONTREAL\n",
    "            data..\n",
    "        CITY=NY\n",
    "            data..\n",
    "\n",
    "    ...\n",
    "      \n",
    "```  \n",
    "\n",
    "https://blog.datasyndrome.com/python-and-parquet-performance-e71da65269ce\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e882",
   "metadata": {},
   "source": [
    "### PARQUET PROS and CONS\n",
    "\n",
    "* Pros: \n",
    " * Highly compressable and since data is stored columnn-wise (compression rates up to 75%)\n",
    "   * can use different compression algorithm with different datatypes\n",
    " * Seamless splittability across columns.\n",
    " * Optimized for reading data and idea for read-intensive tasks\n",
    "   * Can use parallelization to read different column.\n",
    " * Data is self-describing, i.e., schema is included in with the data\n",
    "\n",
    "\n",
    "* Cons:\n",
    " * Very slow at writing data and not good with write-intensive applications\n",
    " * Does not suppport updates on the data as Parquet files are immutable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69d0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
